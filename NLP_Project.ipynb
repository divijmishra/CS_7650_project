{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers accelerate seqeval -U"
      ],
      "metadata": {
        "id": "wJXOcKK60ix9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK1 : Classification (Drug Related/Not Related) using BERT and Clinical BERT"
      ],
      "metadata": {
        "id": "3caZyZsHdss5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model selection\n",
        "# model_checkpoint = \"google-bert/bert-base-uncased\"  # BERT-base\n",
        "model_checkpoint = \"dmis-lab/biobert-v1.1\"  # BioBERT\n",
        "# model_checkpoint = \"medicalai/ClinicalBERT\"  # ClinicalBERT\n",
        "# model_checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"  # Bio-ClinicalBERT"
      ],
      "metadata": {
        "id": "puGgP0O_yTmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVIb_SFkXxjS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Set up the device for GPU usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Perform simple text cleaning operations:\n",
        "    - Lowercasing\n",
        "    - Removing non-alphanumeric characters\n",
        "    \"\"\"\n",
        "    text = text.lower()  # Lowercase text\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    return text\n",
        "\n",
        "def custom_split(dataset, train_frac=0.8, val_frac=0.1, seed=42):\n",
        "    \"\"\"\n",
        "    Custom function to split the dataset into train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    total_size = len(dataset)\n",
        "    indices = np.random.permutation(total_size)\n",
        "    train_size = int(total_size * train_frac)\n",
        "    val_size = int(total_size * val_frac)\n",
        "\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:train_size + val_size]\n",
        "    test_indices = indices[train_size + val_size:]\n",
        "\n",
        "    print(f\"Train size: {len(train_indices)}\")\n",
        "    print(f\"Val size: {len(val_indices)}\")\n",
        "    print(f\"Test size: {len(test_indices)}\")\n",
        "\n",
        "    train_dataset = dataset.select(train_indices)\n",
        "    val_dataset = dataset.select(val_indices)\n",
        "    test_dataset = dataset.select(test_indices)\n",
        "\n",
        "    return DatasetDict({\n",
        "        'train': train_dataset,\n",
        "        'validation': val_dataset,\n",
        "        'test': test_dataset\n",
        "    })\n",
        "\n",
        "# Load the dataset\n",
        "raw_dataset = load_dataset(\"ade_corpus_v2\", \"Ade_corpus_v2_classification\")\n",
        "\n",
        "# Split the data using the custom function\n",
        "split_datasets = custom_split(raw_dataset['train'])\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Tokenize and clean the datasets\n",
        "def preprocess_data(examples):\n",
        "    examples['text'] = [clean_text(text) for text in examples['text']]\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "# Apply preprocessing\n",
        "split_datasets = split_datasets.map(preprocess_data, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute accuracy of the model\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}"
      ],
      "metadata": {
        "id": "d1R9bebaEFGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M82Tl7eGytaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# Define training arguments with logging directory\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./content/drive/My Drive/results\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,  # Evaluate every 100 steps\n",
        "    logging_dir=\"./content/drive/My Drive/logs\",  # Save logs in this directory\n",
        "    logging_steps=100,  # Log metrics every 100 steps\n",
        "    save_steps=500,  # Save the model every 500 steps\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        ")\n",
        "\n",
        "# Assume compute_metrics function is defined\n",
        "def compute_metrics(eval_pred):\n",
        "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "# Initialize Trainer with the compute_metrics function\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=split_datasets['train'],\n",
        "    eval_dataset=split_datasets['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "results = trainer.evaluate(split_datasets['test'])\n",
        "print(\"Test Evaluation results:\", results)\n"
      ],
      "metadata": {
        "id": "rWPjgsQJYNyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = [log['loss'] for log in trainer.state.log_history if 'loss' in log and 'eval_loss' not in log]\n",
        "validation_loss = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log][:-1]\n",
        "epochs = [log['epoch'] for log in trainer.state.log_history if 'loss' in log and 'eval_loss' not in log]\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(10, 5))  # Set the figure size\n",
        "plt.plot(epochs, training_loss, label='Training Loss')  # Plot training loss\n",
        "plt.plot(epochs, validation_loss, label='Validation Loss')  # Plot validation loss\n",
        "plt.title('Training and Validation Loss')  # Title of the plot\n",
        "# plt.xlabel('Training steps / 100')  # Label for the x-axis\n",
        "plt.xlabel('Epochs')  # Label for the x-axis\n",
        "plt.ylabel('Loss')  # Label for the y-axis\n",
        "plt.legend()  # Add a legend\n",
        "plt.grid(True)  # Show grid\n",
        "plt.show()  # Display the plot\n"
      ],
      "metadata": {
        "id": "O89YI91UjXcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume trainer.state.log_history contains your log data\n",
        "accuracy = [log['eval_accuracy'] for log in trainer.state.log_history if 'eval_accuracy' in log][:-1]\n",
        "f1_score = [log['eval_f1'] for log in trainer.state.log_history if 'eval_f1' in log][:-1]\n",
        "epochs = [log['epoch'] for log in trainer.state.log_history if 'eval_f1' in log][:-1]\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting accuracy and F1-score\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Subplot for accuracy\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
        "plt.plot(epochs, accuracy, label='Accuracy', color='blue')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Subplot for F1-score\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
        "plt.plot(epochs, f1_score, label='F1 Score', color='green')\n",
        "plt.title('Validation F1 Score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to not overlap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "X3vORblDqOMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    \"Accuracy\": results[\"eval_accuracy\"],\n",
        "    \"F1 Score\": results[\"eval_f1\"],\n",
        "    \"Precision\": results[\"eval_precision\"],\n",
        "    \"Recall\": results[\"eval_recall\"]\n",
        "}\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame([metrics])\n",
        "\n",
        "# Set more friendly column names if desired\n",
        "df.columns = ['Accuracy', 'F1 Score', 'Precision', 'Recall']\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "JMvnjgON4dgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'trainer' is already initialized and the model is trained\n",
        "predictions = trainer.predict(split_datasets['test'])\n",
        "predictions"
      ],
      "metadata": {
        "id": "oqQuFMCOLs7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Softmax function to convert logits to probabilities\n",
        "def softmax(logits):\n",
        "    e = np.exp(logits - np.max(logits))\n",
        "    return e / e.sum(axis=-1, keepdims=True)\n",
        "\n",
        "# Apply softmax to convert logits into probabilities\n",
        "probabilities = softmax(predictions.predictions)\n",
        "\n",
        "# Get the predicted labels\n",
        "predicted_labels = np.argmax(probabilities, axis=1)\n"
      ],
      "metadata": {
        "id": "iriT2dHVOdGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of examples to display\n",
        "num_examples = 10\n",
        "\n",
        "print(\"Prediction | True Label | Text\")\n",
        "print(\"--------------------------------\")\n",
        "for i in range(num_examples):\n",
        "    true_label = predictions.label_ids[i]\n",
        "    pred_label = predicted_labels[i]\n",
        "    # Get the original text for visualization\n",
        "    text = tokenizer.decode(split_datasets['test'][i]['input_ids'], skip_special_tokens=True)\n",
        "    print(f\"{pred_label}        | {true_label}         | {text}\")\n"
      ],
      "metadata": {
        "id": "CKST3XqIOgt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 2: NER for ADR Relation Extraction using Clinical BERT\n"
      ],
      "metadata": {
        "id": "S7x9KFywdhMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model selection\n",
        "# model_checkpoint = \"google-bert/bert-base-uncased\"  # BERT-base\n",
        "model_checkpoint = \"dmis-lab/biobert-v1.1\"  # BioBERT\n",
        "# model_checkpoint = \"medicalai/ClinicalBERT\"  # ClinicalBERT\n",
        "# model_checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"  # Bio-ClinicalBERT"
      ],
      "metadata": {
        "id": "xeOTPe3Om2Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the dataset\n",
        "raw_dataset = load_dataset(\"ade_corpus_v2\", \"Ade_corpus_v2_drug_ade_relation\")\n",
        "\n",
        "# Split the data using the custom function\n",
        "split_datasets = custom_split(raw_dataset['train'])\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def preprocess_for_ner(texts, drugs, effects, tokenizer):\n",
        "    tokenized_inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "    labels = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective words\n",
        "        label = [\"O\"] * len(word_ids)  # Initialize labels as 'O' for each token\n",
        "\n",
        "        # Assign labels based on the entity matches\n",
        "        def assign_labels(entity_tokens, label_prefix):\n",
        "            sequence = tokenized_inputs.input_ids[i].tolist()  # Get the list of input_ids\n",
        "            for start_index in range(len(sequence)):\n",
        "                if sequence[start_index:start_index+len(entity_tokens)] == entity_tokens:\n",
        "                    label[start_index] = f'B-{label_prefix}'\n",
        "                    for idx in range(start_index + 1, start_index + len(entity_tokens)):\n",
        "                        if idx < len(label):\n",
        "                            label[idx] = f'I-{label_prefix}'\n",
        "\n",
        "        # Tokenize entities\n",
        "        drug_tokens = tokenizer(drugs[i], add_special_tokens=False)['input_ids']\n",
        "        effect_tokens = tokenizer(effects[i], add_special_tokens=False)['input_ids']\n",
        "\n",
        "        # Assign labels for drug and effect\n",
        "        assign_labels(drug_tokens, 'DRUG')\n",
        "        assign_labels(effect_tokens, 'EFFECT')\n",
        "\n",
        "        labels.append(label)\n",
        "\n",
        "    return tokenized_inputs, labels"
      ],
      "metadata": {
        "id": "rSu5bKQdOk-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "texts = [entry['text'] for entry in split_datasets['train']]\n",
        "drugs = [entry['drug'] for entry in split_datasets['train']]\n",
        "effects = [entry['effect'] for entry in split_datasets['train']]\n",
        "print(len(texts))  # This should match the expected number of texts\n",
        "print(texts[0])\n",
        "# Preprocess for NER\n",
        "tokenized_inputs, labels = preprocess_for_ner(texts, drugs, effects, tokenizer)"
      ],
      "metadata": {
        "id": "sVysCZrFSFhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(f\"Text: {texts[i]}\")\n",
        "    print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokenized_inputs.input_ids[i]))\n",
        "    print(\"Labels:\", labels[i])\n",
        "    print(\"actual drug:\", drugs[i])\n",
        "    print(\"actual effect:\", effects[i])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "K3MXat95VymJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "metadata": {
        "id": "3r6oW5N8fAmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting labels from text to indices\n",
        "label_dict = {'O': 0, 'B-DRUG': 1, 'I-DRUG': 2, 'B-EFFECT': 3, 'I-EFFECT': 4}\n",
        "labels = [[label_dict[label] for label in doc] for doc in labels]\n",
        "# Create reverse label dictionary\n",
        "index_to_label = {idx: label for label, idx in label_dict.items()}\n",
        "# Creating the dataset\n",
        "train_labels = labels\n",
        "train_dataset = NERDataset(tokenized_inputs, train_labels)\n"
      ],
      "metadata": {
        "id": "BiOtrX1qnFdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs[:5]"
      ],
      "metadata": {
        "id": "JrpzcdXbnlS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_dict))"
      ],
      "metadata": {
        "id": "JS-WpKLmnKhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "-mn2sRWth0vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(texts, drugs, effects, tokenizer):\n",
        "    tokenized_inputs = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "    labels = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective words\n",
        "        label = [\"O\"] * len(word_ids)  # Initialize labels as 'O' for each token\n",
        "\n",
        "        def assign_labels(entity_tokens, label_prefix):\n",
        "            sequence = tokenized_inputs.input_ids[i].tolist()\n",
        "            for start_index in range(len(sequence)):\n",
        "                if sequence[start_index:start_index+len(entity_tokens)] == entity_tokens:\n",
        "                    label[start_index] = f'B-{label_prefix}'\n",
        "                    for idx in range(start_index + 1, start_index + len(entity_tokens)):\n",
        "                        if idx < len(label):\n",
        "                            label[idx] = f'I-{label_prefix}'\n",
        "\n",
        "        drug_tokens = tokenizer(drugs[i], add_special_tokens=False)['input_ids']\n",
        "        effect_tokens = tokenizer(effects[i], add_special_tokens=False)['input_ids']\n",
        "\n",
        "        assign_labels(drug_tokens, 'DRUG')\n",
        "        assign_labels(effect_tokens, 'EFFECT')\n",
        "\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert labels from text to indices\n",
        "    label_dict = {'O': 0, 'B-DRUG': 1, 'I-DRUG': 2, 'B-EFFECT': 3, 'I-EFFECT': 4}\n",
        "    labels = [[label_dict[lbl] for lbl in doc] for doc in labels]\n",
        "\n",
        "    return NERDataset(tokenized_inputs, labels)\n",
        "\n",
        "# Prepare training, validation, and testing datasets\n",
        "train_texts = [entry['text'] for entry in split_datasets['train']]\n",
        "train_drugs = [entry['drug'] for entry in split_datasets['train']]\n",
        "train_effects = [entry['effect'] for entry in split_datasets['train']]\n",
        "\n",
        "validation_texts = [entry['text'] for entry in split_datasets['validation']]\n",
        "validation_drugs = [entry['drug'] for entry in split_datasets['validation']]\n",
        "validation_effects = [entry['effect'] for entry in split_datasets['validation']]\n",
        "\n",
        "test_texts = [entry['text'] for entry in split_datasets['test']]\n",
        "test_drugs = [entry['drug'] for entry in split_datasets['test']]\n",
        "test_effects = [entry['effect'] for entry in split_datasets['test']]\n",
        "\n",
        "train_dataset = prepare_dataset(train_texts, train_drugs, train_effects, tokenizer)\n",
        "validation_dataset = prepare_dataset(validation_texts, validation_drugs, validation_effects, tokenizer)\n",
        "test_dataset = prepare_dataset(test_texts, test_drugs, test_effects, tokenizer)\n",
        "\n",
        "print(f\"Train Dataset: {len(train_dataset)}\")\n",
        "print(f\"Validation Dataset: {len(validation_dataset)}\")\n",
        "print(f\"Test Dataset: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "FkWHnJPwoIT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from seqeval.scheme import IOB2\n",
        "\n",
        "def align_predictions(predictions, label_ids):\n",
        "    preds = np.argmax(predictions, axis=2)\n",
        "    batch_size, seq_len = preds.shape\n",
        "    out_label_list = [[] for _ in range(batch_size)]\n",
        "    preds_list = [[] for _ in range(batch_size)]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for j in range(seq_len):\n",
        "            if label_ids[i, j] != torch.nn.CrossEntropyLoss().ignore_index:\n",
        "                out_label_list[i].append(index_to_label.get(label_ids[i][j], 'O'))\n",
        "                preds_list[i].append(index_to_label.get(preds[i][j], 'O'))\n",
        "\n",
        "    return preds_list, out_label_list  # Return lists of lists, one per sentence\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions, true_labels = align_predictions(predictions, labels)\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, predictions),\n",
        "        \"recall\": recall_score(true_labels, predictions),\n",
        "        \"f1\": f1_score(true_labels, predictions)\n",
        "    }"
      ],
      "metadata": {
        "id": "91uOHroe1ueW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./content/drive/My Drive/ner_results\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=20,  # Evaluate every 100 steps\n",
        "    logging_dir=\"./content/drive/My Drive/ner_logs\",  # Save logs in this directory\n",
        "    logging_steps=20,  # Log metrics every 100 steps\n",
        "    save_steps=1000,  # Save the model every 500 steps\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    compute_metrics=compute_metrics  # Set the compute_metrics function\n",
        ")\n"
      ],
      "metadata": {
        "id": "VAiJNw9T2BSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "O3HcC8p72uZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"Test Results:\", test_results)"
      ],
      "metadata": {
        "id": "KnIoc80f3WGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = [log['loss'] for log in trainer.state.log_history if 'loss' in log and 'eval_loss' not in log]\n",
        "validation_loss = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log][:-1]\n",
        "epochs = [log['epoch'] for log in trainer.state.log_history if 'loss' in log and 'eval_loss' not in log]\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(10, 5))  # Set the figure size\n",
        "plt.plot(epochs, training_loss, label='Training Loss')  # Plot training loss\n",
        "plt.plot(epochs, validation_loss, label='Validation Loss')  # Plot validation loss\n",
        "plt.title('Training and Validation Loss')  # Title of the plot\n",
        "plt.xlabel('Epochs')  # Label for the x-axis\n",
        "plt.ylabel('Loss')  # Label for the y-axis\n",
        "plt.legend()  # Add a legend\n",
        "plt.grid(True)  # Show grid\n",
        "plt.show()  # Display the plot\n"
      ],
      "metadata": {
        "id": "B8013c-KFI2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume trainer.state.log_history contains your log data\n",
        "precision = [log['eval_precision'] for log in trainer.state.log_history if 'eval_precision' in log][:-1]\n",
        "recall = [log['eval_recall'] for log in trainer.state.log_history if 'eval_recall' in log][:-1]\n",
        "f1_score = [log['eval_f1'] for log in trainer.state.log_history if 'eval_f1' in log][:-1]\n",
        "epochs = [log['epoch'] for log in trainer.state.log_history if 'eval_f1' in log][:-1]\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting accuracy and F1-score\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Subplot for accuracy\n",
        "plt.subplot(1, 3, 1)  # 1 row, 3 columns, 1st subplot\n",
        "plt.plot(epochs, precision, label='precision', color='blue')\n",
        "plt.title('Validation precision')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('precision')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Subplot for F1-score\n",
        "plt.subplot(1, 3, 2)  # 1 row, 3 columns, 2nd subplot\n",
        "plt.plot(epochs, recall, label='recall', color='red')\n",
        "plt.title('Validation recall')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('recall')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)  # 1 row, 3 columns, 3nd subplot\n",
        "plt.plot(epochs, f1_score, label='F1 Score', color='green')\n",
        "plt.title('Validation F1 Score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to not overlap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "va77C8oVF-fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    \"Precision\": test_results[\"eval_precision\"],\n",
        "    \"Recall\": test_results[\"eval_recall\"],\n",
        "    \"F1 Score\": test_results[\"eval_f1\"],\n",
        "}\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame([metrics])\n",
        "\n",
        "# Set more friendly column names if desired\n",
        "df.columns = [ 'Precision', 'Recall', 'F1 Score']\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "aaeCihP6VY9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'trainer' is already initialized and the model is trained\n",
        "predictions, labels, _ = trainer.predict(test_dataset)"
      ],
      "metadata": {
        "id": "3gHd7cXXWZfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, true_labels = align_predictions(predictions, labels)"
      ],
      "metadata": {
        "id": "57EP7O_WWm75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_sequences = [tokenizer.convert_ids_to_tokens(ids) for ids in test_dataset.encodings['input_ids']]"
      ],
      "metadata": {
        "id": "70iQhFrRYvvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 5 random samples (or just the first five if randomness isn't needed)\n",
        "for i in range(5):\n",
        "    print(f\"Text: {token_sequences[i]}\")\n",
        "    print(f\"True Labels: {true_labels[i]}\")\n",
        "    print(f\"Predicted Labels: {predictions[i]}\\n\")"
      ],
      "metadata": {
        "id": "n7sk_ZI-Y5jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YTjMmVr5Y93A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}